{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "042de7b8-9481-4e93-83a5-5e116faefa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils import (generate_prompt, \n",
    "                   generate_test_prompt, \n",
    "                    find_all_linear_names, \n",
    "                    predict_domain,\n",
    "                    predict, \n",
    "                    evaluate, \n",
    "                    load_pretrained_model, \n",
    "                    generate_train_val_data,\n",
    "                    initiate_trainer,\n",
    "                    initiate_base_model)\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "568e7ced-83f1-4957-a8a5-b1be0febf2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correcting_semantic_type(semantic_type):\n",
    "    return semantic_type.replace(',',';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84603410-d176-46be-b8ff-27b0216a2714",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cta_types_domain_reduced_5domain.json', 'r') as file:\n",
    "    cta_type_domain = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a67ce9ef-7202-414e-8042-a7ceacb45eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_prompt_NextiaJD_predict_domain(table):\n",
    "    with open('cta_types_domain_reduced_5domain.json', 'r') as file:\n",
    "        cta_type_domain = json.load(file)\n",
    "    return f\"\"\"\n",
    "            Answer the question based on the task and instructions below. If the question cannot be answered using the information provided answer with \"Place\".\n",
    "            Task: Classify the table given to you with only one of the following domains that are separated with comma: {\", \".join(cta_type_domain.keys())}.\n",
    "            Instructions: 1. Look at the input given to you. 2. Look at the cell values in detail. 3. Decide if describes a {\", \".join(cta_type_domain.keys())}. 4. Answer only with the predicted domain. \n",
    "            Example 1: Table: [[\"Friends Pizza\", 2525, Cash Visa MasterCard, 7:30 AM]]\n",
    "            Domain: Restaurant\n",
    "            Example 2: Table: [[Museum/Gallery, Vancouver; BC; V6J 2C7, Kitsilano]]\n",
    "            Domain: Place\n",
    "            Table: {table.iloc[:30,:].values}\n",
    "            Domain: \"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a64ba00b-8e7a-45d2-b823-fe4414f0d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_prompt_NextiaJD_few_shot(data_point, domain):\n",
    "    with open('cta_types_domain_reduced_5domain.json', 'r') as file:\n",
    "        cta_type_domain = json.load(file)\n",
    "    return f\"\"\"\n",
    "            Answer the question based on the task, instructions and examples below. If the question cannot be answered using the information provided answer with \"I don't know\".\n",
    "            Task: Classify the text given to you with one of these classes that are separated with comma: {\", \".join(cta_type_domain[domain])}.\n",
    "            Instructions: 1. Look at the input given to you. 2. Look at the cell values in detail.\n",
    "            Example 1: Column: [Kitsilano, Strathcona, Downtown, UBC, Downtown, Mount Pleasant]\n",
    "            label: addressLocality\n",
    "            Example 2: Column: ['www.memorybc.ca/museum-of-15th-field-artillery-regiment','www.221a.ca/', 'https://www.facebook.com/ACMEstudiobuilding','http://gallery.ahva.ubc.ca/about/','http://www.mozaicoflamenco.com/', 'http://www.anzaclub.org','www.artbeatus.com', 'http://www.artsfactorysociety.ca/']\n",
    "            label: URL\n",
    "            Column: {data_point.values}\n",
    "            label: \"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01bb125c-1803-46e1-969e-f5f69b476330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(set1, set2):\n",
    "    \n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "     \n",
    "    return intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14637e3a-68ec-4c69-89db-a9632a213009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(dataframe, model, tokenizer):\n",
    "    X_test_domain = generate_test_prompt_NextiaJD_predict_domain(dataframe)\n",
    "    domain = predict_domain(X_test_domain, model, tokenizer)\n",
    "    print(domain)\n",
    "    X_test = pd.DataFrame(dataframe.iloc[:30].apply(generate_test_prompt_NextiaJD_few_shot, args=(domain,), axis=0), columns=[\"prediction\"])\n",
    "    y_pred = predict(X_test, model, tokenizer, domain)\n",
    "    predictions = pd.DataFrame({\n",
    "                                \"Column\":dataframe.columns, \n",
    "                                \"Predicted\":y_pred\n",
    "                                })\n",
    "    return predictions, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75ed1d42-53b8-415c-950b-d6228c395380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_possible_joinable_columns(predictions_left, predictions_right, filename_left, filename_right):\n",
    "    predicted_joinable_columns = []\n",
    "\n",
    "    for i in range(predictions_left.shape[0]):\n",
    "        column = predictions_left.iloc[i,0]\n",
    "        predicted_semantic_type = predictions_left.iloc[i,1]\n",
    "        for j in range(predictions_right.shape[0]):\n",
    "            if(predicted_semantic_type == predictions_right.iloc[j,1]):\n",
    "                predicted_joinable_columns.append([filename_left, column,\n",
    "                                                   filename_right, predictions_right.iloc[j,:].Column, \n",
    "                                                   predicted_semantic_type])\n",
    "\n",
    "    return predicted_joinable_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ff47cf3-5957-4872-908b-671772cf17ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarities(sameSTColumns, df_left, df_right):\n",
    "    similarity = []\n",
    "    for i in range(sameSTColumns.shape[0]):\n",
    "        try:\n",
    "            similarity.append(jaccard_similarity(set(df_left[sameSTColumns.iloc[i,1]].unique()), set(df_right[sameSTColumns.iloc[i,3]].unique())))\n",
    "        except:\n",
    "            similarity.append(jaccard_similarity(set(df_right[sameSTColumns.iloc[i,1]].unique()), set(df_left[sameSTColumns.iloc[i,3]].unique())))\n",
    "\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bbfa53d-e843-42c3-9e35-964e0a86bd2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['community-centres.csv', 'community-gardens-and-food-trees.csv',\n",
       "       'cultural-spaces.csv', 'eo4.csv', 'eo_pr.csv', 'eo_xx.csv',\n",
       "       'libraries.csv', 'population-census-of-botswana-2011.csv',\n",
       "       'public-art-artists.csv', 'public-art.csv',\n",
       "       'rental-standards-current-issues.csv', 'schools.csv',\n",
       "       'statewise-census-data-in-india-1901-2011.csv',\n",
       "       'street-intersections.csv'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_joinable_columns = pd.read_csv('joinable_columns_75containment.csv')\n",
    "all_joinable_files = np.concatenate((df_joinable_columns['ds_name'], df_joinable_columns['ds_name_2']), axis=0)\n",
    "all_joinable_files = np.unique(all_joinable_files)\n",
    "all_joinable_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea89647c-4539-469f-9ddc-8ecf85a19b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dsInformation = pd.read_csv('datasetInformation_testbedXS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3fe0a8a-775b-41bb-86e3-f5b26181c909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aebd6fa7e6248298846b6dde64fe2b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model_name = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "model, tokenizer = initiate_base_model(base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57495bbd-8ccf-4736-beb1-5726c79a3597",
   "metadata": {},
   "outputs": [],
   "source": [
    "modules = find_all_linear_names(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6500b1d0-fd5c-4160-9228-d8248f697de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions for table community-centres.csv. 1 out of 14\n",
      "Place\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:03<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions for table community-gardens-and-food-trees.csv. 2 out of 14\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.65 GiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m info  \u001b[38;5;241m=\u001b[39m df_dsInformation[df_dsInformation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m all_joinable_files[i]]\n\u001b[1;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mall_joinable_files[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, delimiter\u001b[38;5;241m=\u001b[39minfo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m----> 6\u001b[0m prediction, X_test \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m ST_predictions_dict[all_joinable_files[i]] \u001b[38;5;241m=\u001b[39m prediction\n",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m, in \u001b[0;36mgenerate_predictions\u001b[0;34m(dataframe, model, tokenizer)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_predictions\u001b[39m(dataframe, model, tokenizer):\n\u001b[1;32m      2\u001b[0m     X_test_domain \u001b[38;5;241m=\u001b[39m generate_test_prompt_NextiaJD_predict_domain(dataframe)\n\u001b[0;32m----> 3\u001b[0m     domain \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_domain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_domain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(domain)\n\u001b[1;32m      5\u001b[0m     X_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(dataframe\u001b[38;5;241m.\u001b[39miloc[:\u001b[38;5;241m30\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(generate_test_prompt_NextiaJD_few_shot, args\u001b[38;5;241m=\u001b[39m(domain,), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/cta/testbedXS/utils.py:67\u001b[0m, in \u001b[0;36mpredict_domain\u001b[0;34m(prompt, model, tokenizer)\u001b[0m\n\u001b[1;32m     59\u001b[0m     cta_type_domain \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[1;32m     61\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     62\u001b[0m                 model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     63\u001b[0m                 tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     64\u001b[0m                 max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     65\u001b[0m                 temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m---> 67\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m answer \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDomain:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m answer\n",
      "File \u001b[0;32m~/anaconda3/envs/sotab/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:262\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/sotab/lib/python3.10/site-packages/transformers/pipelines/base.py:1257\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1250\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1251\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m         )\n\u001b[1;32m   1255\u001b[0m     )\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/sotab/lib/python3.10/site-packages/transformers/pipelines/base.py:1264\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1263\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1264\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/sotab/lib/python3.10/site-packages/transformers/pipelines/base.py:1164\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1163\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1164\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/sotab/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:351\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/sotab/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/sotab/lib/python3.10/site-packages/transformers/generation/utils.py:2024\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2016\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2017\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2018\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2019\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2020\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2021\u001b[0m     )\n\u001b[1;32m   2023\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2024\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2032\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2036\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   2037\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2038\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[1;32m   2040\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2041\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/sotab/lib/python3.10/site-packages/transformers/generation/utils.py:2982\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2979\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   2981\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2982\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2985\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/sotab/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/sotab/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/sotab/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/sotab/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1209\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1208\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n\u001b[0;32m-> 1209\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1211\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;66;03m# Shift so that tokens < n predict n\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.65 GiB. GPU "
     ]
    }
   ],
   "source": [
    "ST_predictions_dict = {}\n",
    "for i in range(len(all_joinable_files)):\n",
    "    print(f'Making predictions for table {all_joinable_files[i]}. {i+1} out of {len(all_joinable_files)}')\n",
    "    info  = df_dsInformation[df_dsInformation['filename'] == all_joinable_files[i]]\n",
    "    df = pd.read_csv(f'datasets/{all_joinable_files[i]}', delimiter=info['delimiter'].values[0])\n",
    "    prediction, X_test = generate_predictions(df, model, tokenizer)\n",
    "    ST_predictions_dict[all_joinable_files[i]] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526515e8-1ad2-41c4-b43f-b95d1c368c7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ST_predictions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9992fc-cf70-468f-a00a-528dea42b4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e5f7c4-3baa-4ee4-9be0-dbac201a62df",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153bd338-298c-4be3-b782-f12b32758ab3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_predicted_joinable_columns = pd.DataFrame([])\n",
    "similarity_calculations = 0\n",
    "brute_force_calculations = 0\n",
    "\n",
    "for i in range(len(all_joinable_files)-1):\n",
    "    left_info  = df_dsInformation[df_dsInformation['filename'] == all_joinable_files[i]]\n",
    "    df_left = pd.read_csv(f'datasets/{all_joinable_files[i]}', delimiter=left_info['delimiter'].values[0])\n",
    "    \n",
    "    for j in range(i+1, len(all_joinable_files)):\n",
    "        print(f'Calculating Similarities for tables {all_joinable_files[i]} and {all_joinable_files[j]}.')\n",
    "        \n",
    "        right_info = df_dsInformation[df_dsInformation['filename'] == all_joinable_files[j]]\n",
    "        df_right = pd.read_csv(f'datasets/{all_joinable_files[j]}', delimiter=right_info['delimiter'].values[0])\n",
    "    \n",
    "        predictions_left = ST_predictions_dict[all_joinable_files[i]]\n",
    "        predictions_right = ST_predictions_dict[all_joinable_files[j]]\n",
    "    \n",
    "        predicted_joinable_columns = generate_possible_joinable_columns(predictions_left, predictions_right, all_joinable_files[i], all_joinable_files[j])\n",
    "    \n",
    "        try:\n",
    "            sameSTColumns = pd.DataFrame(np.array(predicted_joinable_columns), columns=['FilenameLeft', 'ColumnLeft', \n",
    "                                                                                        'FilenameRight','ColumnRight',\n",
    "                                                                                        'SemanticType'])\n",
    "        except ValueError:\n",
    "            print('No matches found, skipping to next column.')\n",
    "            continue\n",
    "            \n",
    "        similarity = calculate_similarities(sameSTColumns, df_left, df_right)\n",
    "        sameSTColumns['JaccardSimilarity'] = similarity\n",
    "        joinableColumns = sameSTColumns[sameSTColumns['JaccardSimilarity'] >= 0.75]\n",
    "\n",
    "        similarity_calculations += sameSTColumns.shape[0]\n",
    "        brute_force_calculations += df_left.shape[1] * df_right.shape[1]\n",
    "        \n",
    "        if(len(joinableColumns) > 0):\n",
    "            print(f'Adding {joinableColumns.shape[0]} columns')\n",
    "        \n",
    "        if(len(all_predicted_joinable_columns) == 0):\n",
    "            all_predicted_joinable_columns = joinableColumns\n",
    "        else:\n",
    "            all_predicted_joinable_columns = pd.concat((all_predicted_joinable_columns, joinableColumns), axis=0)\n",
    "            print(f'New size {all_predicted_joinable_columns.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7decddbf-ca1a-41a5-a267-1fd78de453b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(similarity_calculations, brute_force_calculations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793087fe-026f-4e06-a014-e022873eddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predicted_joinable_columns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215b9373-db49-4d31-8824-0ce085ebce84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(all_joinable_files)-1):   \n",
    "#     for j in range(i+1, len(all_joinable_files)):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b31ba-b41c-4d82-a608-e967d81fc52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predicted_joinable_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27467ec1-e429-4883-9b4e-9d08dd6b1400",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joinable_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa3f30-5da5-421a-b3e5-6b26a4ca33d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joinable_columns[df_joinable_columns['ds_name'] == 'cultural-spaces.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168aa2d8-644d-4685-82e8-e2da4dfdef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predicted_joinable_columns[(all_predicted_joinable_columns['FilenameRight'] == 'cultural-spaces.csv') | (all_predicted_joinable_columns['FilenameLeft'] == 'cultural-spaces.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ec5311-a9e1-4861-aa07-d2ab20078752",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_predicted_joinable_columns_joins = []\n",
    "for i in range(len(all_predicted_joinable_columns)):\n",
    "    all_predicted_joinable_columns_joins.append(';'.join(all_predicted_joinable_columns.iloc[i,:4].values))\n",
    "all_predicted_joinable_columns_joins = np.array(all_predicted_joinable_columns_joins)\n",
    "all_predicted_joinable_columns_joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cce044-5e60-4464-9e57-c068d7d95ae3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_joinable_columns_joins = []\n",
    "for i in range(len(df_joinable_columns)):\n",
    "    df_joinable_columns_joins.append(';'.join(df_joinable_columns.iloc[i,:4].values))\n",
    "    df_joinable_columns_joins.append(';'.join(df_joinable_columns.iloc[i,2:].values)+';'+';'.join(df_joinable_columns.iloc[i,:2].values))\n",
    "df_joinable_columns_joins = np.array(df_joinable_columns_joins)\n",
    "df_joinable_columns_joins = np.unique(df_joinable_columns_joins)\n",
    "df_joinable_columns_joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a319aeb-9ec3-45f8-a516-d74a7c575d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predicted_joinable_columns_joins.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86e5ab5-dfc1-4653-8888-c2f76d933001",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joinable_columns_joins.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb892e58-8de6-4296-9abc-e4804e1a4b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_similarity(set(all_predicted_joinable_columns_joins), set(df_joinable_columns_joins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753d6e64-498a-4742-b368-412eb0a290c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "for i in range(len(all_predicted_joinable_columns_joins)):\n",
    "    if(all_predicted_joinable_columns_joins[i] in df_joinable_columns_joins):\n",
    "        tp += 1\n",
    "    else:\n",
    "        fp += 1\n",
    "    fn = len(df_joinable_columns_joins) - (tp+fp)\n",
    "\n",
    "precision = (tp)/(tp+fp)\n",
    "recall = (tp)/(tp+fn)\n",
    "f1_score = 2 * (precision*recall)/(precision+recall)\n",
    "\n",
    "print(precision, recall, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21fd3b4-233d-4a57-8e28-a91a7815fe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "ST_predictions_dict['eo4.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe5668e-be6b-4aee-9825-e491a313d939",
   "metadata": {},
   "outputs": [],
   "source": [
    "ST_predictions_dict['eo_pr.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149efe97-b27c-4c49-a178-ed770b7f4106",
   "metadata": {},
   "outputs": [],
   "source": [
    "ST_predictions_dict['cultural-spaces.csv']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
